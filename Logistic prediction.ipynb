{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IMCJ7qTo1rkb"},"outputs":[],"source":["#Importing the libraries\n","import math,copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3165,"status":"ok","timestamp":1680240019616,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"},"user_tz":-330},"id":"kbh4iFv_167a","outputId":"b26fcefe-1567-4d32-e451-53f453956cb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] [8 4 1 ... 9 4 6]\n"]}],"source":["#Bringing dataset in desired form and defining ultravariables\n","my_data = pd.read_csv('classification_train.csv') #read the data\n","x = my_data.iloc[:,2:]/255\n","X_train=x.to_numpy()\n","Y = my_data.iloc[:,1]\n","y_train=Y.to_numpy()\n","w_init=np.zeros(len(X_train[0]))\n","b_init=0.0\n","print(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1353,"status":"ok","timestamp":1680240020962,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"},"user_tz":-330},"id":"KyZ5hbd5QJGZ","outputId":"5ad2ba40-0c52-4d60-d5c3-79080f23a1fb"},"outputs":[{"data":{"text/plain":["array([25672, 59964, 66230, ..., 99226, 58519, 80458])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["#Bringing the test dataset in desired form\n","my_data1 = pd.read_csv('classification_test.csv') #read the data\n","x1 = my_data1.iloc[:,1:]/255\n","X_train1=x1.to_numpy()\n","x2=my_data1.iloc[:,0]\n","ids=x2.to_numpy()\n","ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzP19ENK6HTs"},"outputs":[],"source":["#creating a list #en with labels for specific class wise binary classification\n","m=len(X_train)\n","#en is an array which has separate only 1-0 wala label for every class\n","p=0\n","en=np.zeros((10,len(X_train)))\n","for i in y_train:\n","  for j in np.unique(y_train):\n","    if i==j:\n","      en[j][p]+=1\n","  p+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ja_1GgTu2KyL"},"outputs":[],"source":["#Creating the sigmoid function\n","def sigmoid(z):\n","  g=1/(1+np.exp(-z))\n","  return g"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAMPQLhV2hY3"},"outputs":[],"source":["#Predicting the label\n","fwb=[]\n","m=len(X_train)\n","for i in range(m):\n","    yp=sigmoid(np.dot(X_train[i], w_init)+b_init)\n","    fwb.append(yp)\n","        \n","fwbs=np.array(fwb)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB38iqzSIqPJ"},"outputs":[],"source":["def compute_cost(X, y, w, b, *argv):\n","    \"\"\"\n","    Computes the cost over all examples\n","    Args:\n","      X : (ndarray Shape (m,n)) data, m examples by n features\n","      y : (ndarray Shape (m,))  target value \n","      w : (ndarray Shape (n,))  values of parameters of the model      \n","      b : (scalar)              value of bias parameter of the model\n","      *argv : unused, for compatibility with regularized version below\n","    Returns:\n","      total_cost : (scalar) cost \n","    \"\"\"\n","\n","    m, n = X.shape\n","    \n","    \n","    fwb=[]\n","    for i in range(m):\n","        yp=sigmoid(np.dot(X[i], w)+b)\n","        fwb.append(yp)\n","        \n","    fwbs=np.array(fwb)\n","    \n","    cost=[]\n","    loss=0\n","    for i in range(m):\n","        loss=-y[i]*np.log(fwbs[i])-(1-y[i])*np.log(1-fwbs[i])\n","        cost.append(loss)\n","        \n","    c=np.array(cost)\n","    total_cost=np.sum(c)/m \n","\n","    return total_cost\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb-FTTu0OGZQ"},"outputs":[],"source":["# m, n = X_train.shape\n","\n","# # Compute and display cost with w and b initialized to zeros\n","# initial_w = np.zeros(len(X_train[0]))\n","# initial_b = 0.\n","# cost = compute_cost(X_train, y_train, initial_w, initial_b)\n","# print('Cost at initial w and b (zeros): {:.3f}'.format(cost))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQQKJGZjTGML"},"outputs":[],"source":["# # Compute and display cost with non-zero w and b\n","# np.random.seed(1)\n","# test_w = 0.01 * (np.random.rand(784) - 0.5)\n","# test_b = -24.\n","# cost = compute_cost(X_train, en[0], test_w, test_b)\n","\n","# print('Cost at test w and b (non-zeros): {:.3f}'.format(cost))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJSWq9XYONQX"},"outputs":[],"source":["\n","def compute_gradient(X, y, w, b, *argv): \n","    \"\"\"\n","    Computes the gradient for logistic regression \n"," \n","    Args:\n","      X : (ndarray Shape (m,n)) data, m examples by n features\n","      y : (ndarray Shape (m,))  target value \n","      w : (ndarray Shape (n,))  values of parameters of the model      \n","      b : (scalar)              value of bias parameter of the model\n","      *argv : unused, for compatibility with regularized version below\n","    Returns\n","      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n","      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n","    \"\"\"\n","    m, n = X.shape\n","    dj_dw = np.zeros(w.shape)\n","    dj_db = 0.\n","    djn=np.zeros(1)\n","\n","    ### START CODE HERE ### \n","    fwb=[]\n","    for i in range(m):\n","        yp=sigmoid(np.dot(X[i], w)+b)\n","        fwb.append(yp)\n","        \n","    fwbs=np.array(fwb)\n","    \n","    for j in range(n):    \n","        for i in range(m):\n","            dj_dw[j]+=(fwbs[i]-y[i])*X[i][j]\n","    dj_dw=dj_dw/m\n","    \n","    dj_db=(np.sum(fwbs-y))/m\n","            \n","    \n","    ### END CODE HERE ###\n","\n","        \n","    return dj_db, dj_dw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlYJNRSsOSqA"},"outputs":[],"source":["# # Compute and display gradient with w and b initialized to zeros\n","# initial_w = np.zeros(len(X_train[0]))\n","# initial_b = 0.\n","\n","# dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n","# print(f'dj_db at initial w and b (zeros):{dj_db}' )\n","# print(f'dj_dw at initial w and b (zeros):{dj_dw.tolist()}' )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yYoCHM5VZJp"},"outputs":[],"source":["# # Compute and display cost and gradient with non-zero w and b\n","# np.random.seed(1)\n","# test_w = 0.01 * (np.random.rand(784) - 0.5)\n","# test_b = -24\n","# dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n","\n","# print('dj_db at test w and b:', dj_db)\n","# print('dj_dw at test w and b:', dj_dw.tolist())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvZ3UF26PST-"},"outputs":[],"source":["def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n","    \"\"\"\n","    Performs batch gradient descent to learn theta. Updates theta by taking \n","    num_iters gradient steps with learning rate alpha\n","    \n","    Args:\n","      X :    (ndarray Shape (m, n) data, m examples by n features\n","      y :    (ndarray Shape (m,))  target value \n","      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n","      b_in : (scalar)              Initial value of parameter of the model\n","      cost_function :              function to compute cost\n","      gradient_function :          function to compute gradient\n","      alpha : (float)              Learning rate\n","      num_iters : (int)            number of iterations to run gradient descent\n","      lambda_ : (scalar, float)    regularization constant\n","      \n","    Returns:\n","      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n","          running gradient descent\n","      b : (scalar)                Updated value of parameter of the model after\n","          running gradient descent\n","    \"\"\"\n","    \n","    # number of training examples\n","    m = len(X)\n","    \n","    # An array to store cost J and w's at each iteration primarily for graphing later\n","    J_history = []\n","    w_history = []\n","    \n","    for i in range(num_iters):\n","\n","        # Calculate the gradient and update the parameters\n","        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n","\n","        # Update Parameters using w, b, alpha and gradient\n","        w_in = w_in - alpha * dj_dw               \n","        b_in = b_in - alpha * dj_db              \n","       \n","        # Save cost J at each iteration\n","        if i<100000:      # prevent resource exhaustion \n","            cost =  cost_function(X, y, w_in, b_in, lambda_)\n","            J_history.append(cost)\n","\n","        # Print cost every at intervals 10 times or as many iterations if < 10\n","        if i% math.ceil(num_iters/2) == 0 or i == (num_iters-1):\n","            w_history.append(w_in)\n","            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n","        \n","    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLnukESSPhcm"},"outputs":[],"source":["#Initialising ultravariables\n","np.random.seed(1)\n","initial_w = 0.01 * (np.random.rand(784) - 0.5)\n","initial_b = -8"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGGTfUsQbGRp","executionInfo":{"status":"ok","timestamp":1680250206868,"user_tz":-330,"elapsed":2032873,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}},"outputId":"fad7f2a7-027b-45b7-c505-def7b6be44b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration    0: Cost     0.53   \n","Iteration   25: Cost     0.21   \n","Iteration   49: Cost     0.19   \n","Iteration    0: Cost     0.59   \n","Iteration   25: Cost     0.10   \n","Iteration   49: Cost     0.08   \n","Iteration    0: Cost     0.48   \n","Iteration   25: Cost     0.27   \n","Iteration   49: Cost     0.24   \n","Iteration    0: Cost     0.57   \n","Iteration   25: Cost     0.20   \n","Iteration   49: Cost     0.18   \n","Iteration    0: Cost     0.44   \n","Iteration   25: Cost     0.23   \n","Iteration   49: Cost     0.19   \n","Iteration    0: Cost     0.74   \n","Iteration   25: Cost     0.45   \n","Iteration   49: Cost     0.39   \n","Iteration    0: Cost     0.54   \n","Iteration   25: Cost     0.38   \n","Iteration   49: Cost     0.36   \n","Iteration    0: Cost     0.67   \n","Iteration   25: Cost     0.14   \n","Iteration   49: Cost     0.12   \n","Iteration    0: Cost     0.52   \n","Iteration   25: Cost     0.15   \n","Iteration   49: Cost     0.12   \n","Iteration    0: Cost     0.53   \n","Iteration   25: Cost     0.08   \n","Iteration   49: Cost     0.07   \n"]}],"source":["#Making the prediction matrix.\n","pred=np.zeros((10,len(X_train1)))\n","for i in range(10):\n","  iterations =50\n","  alpha = 0.2\n","  w,b, J_history,_ = gradient_descent(X_train ,en[i], initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations, 0)\n","  \n","  #Creating the probability matrix for each class\n","  fwb=[]\n","  m=len(X_train1)\n","  for j in range(m):\n","    yp=sigmoid(np.dot(X_train1[j], w)+b)\n","    fwb.append(yp)\n","        \n","  fwbs=np.array(fwb)\n","  pred[i]=fwbs\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"MW4xP4sxnI5K","executionInfo":{"status":"ok","timestamp":1680250206872,"user_tz":-330,"elapsed":8,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}}},"outputs":[],"source":["#final prediction\n","predict=pred.transpose()\n","y_final=[]\n","\n","for i in predict:\n","  l=0\n","  for m in range(10):    \n","    if i[l]==max(i):\n","      y_final.append(l)\n","      break\n","    else:\n","      l+=1\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"vOoEIfmxY9bU","executionInfo":{"status":"ok","timestamp":1680250206873,"user_tz":-330,"elapsed":9,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}}},"outputs":[],"source":["from numpy import asarray\n","from numpy import savetxt\n","savetxt('classification_test(1).csv', y_final, delimiter=',', header='Labels')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"IaOEHLZHc-nv","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1680250330448,"user_tz":-330,"elapsed":464,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}},"outputId":"80fc1da6-42c4-42d0-9530-bd7f4fedf1eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      # Labels\n","0          0.0\n","1          1.0\n","2          2.0\n","3          0.0\n","4          1.0\n","...        ...\n","9995       0.0\n","9996       6.0\n","9997       8.0\n","9998       6.0\n","9999       6.0\n","\n","[10000 rows x 1 columns]"],"text/html":["\n","  <div id=\"df-aac061de-e1a0-4f64-8c7f-e889941f20a2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>8.0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>6.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 1 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aac061de-e1a0-4f64-8c7f-e889941f20a2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-aac061de-e1a0-4f64-8c7f-e889941f20a2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-aac061de-e1a0-4f64-8c7f-e889941f20a2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["my_data = pd.read_csv('classification_test.csv')\n","my_data"]},{"cell_type":"markdown","metadata":{"id":"u0KqyGsZsFuJ"},"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1tH7L1CRlvoAswcDHF719DY4EqXy4xKlh","timestamp":1680208461044}],"authorship_tag":"ABX9TyPr5N32MCFkx5rLhAZLxka9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}