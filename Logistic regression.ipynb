{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IMCJ7qTo1rkb"},"outputs":[],"source":["#Importing the libraries\n","import math,copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3914,"status":"ok","timestamp":1680241286644,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"},"user_tz":-330},"id":"kbh4iFv_167a","outputId":"a29a7171-c179-4102-afb8-4b29153c81c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] [8 4 1 ... 9 4 6]\n"]}],"source":["#Bringing dataset in desired form and defining ultravariables\n","my_data = pd.read_csv('classification_train.csv') #read the data\n","x = my_data.iloc[:,2:]/255\n","X_train=x.to_numpy()\n","Y = my_data.iloc[:,1]\n","y_train=Y.to_numpy()\n","w_init=np.zeros(len(X_train[0]))\n","b_init=0.0\n","print(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1680241286645,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"},"user_tz":-330},"id":"FNnC5_GA65h-","outputId":"3522ef3a-6b2d-43ee-e795-9cc69de31b7a"},"outputs":[{"data":{"text/plain":["30000"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["len(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzP19ENK6HTs"},"outputs":[],"source":["#creating a list #en with labels for specific class wise binary classification\n","m=len(X_train)\n","#en is an array which has separate only 1-0 wala label for every class\n","p=0\n","en=np.zeros((10,len(X_train)))\n","for i in y_train:\n","  for j in np.unique(y_train):\n","    if i==j:\n","      en[j][p]+=1\n","  p+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1680241313157,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"},"user_tz":-330},"id":"DB2tJnn3JGqf","outputId":"587b33e2-d62f-45cd-cda0-b30ad9abb7ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 1. ... 0. 0. 0.]\n"]}],"source":["print(en[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ja_1GgTu2KyL"},"outputs":[],"source":["#Creating the sigmoid function\n","def sigmoid(z):\n","  g=1/(1+np.exp(-z))\n","  return g"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAMPQLhV2hY3"},"outputs":[],"source":["#Predicting the label\n","fwb=[]\n","m=len(X_train)\n","for i in range(m):\n","    yp=sigmoid(np.dot(X_train[i], w_init)+b_init)\n","    fwb.append(yp)\n","        \n","fwbs=np.array(fwb)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB38iqzSIqPJ"},"outputs":[],"source":["def compute_cost(X, y, w, b, *argv):\n","    \"\"\"\n","    Computes the cost over all examples\n","    Args:\n","      X : (ndarray Shape (m,n)) data, m examples by n features\n","      y : (ndarray Shape (m,))  target value \n","      w : (ndarray Shape (n,))  values of parameters of the model      \n","      b : (scalar)              value of bias parameter of the model\n","      *argv : unused, for compatibility with regularized version below\n","    Returns:\n","      total_cost : (scalar) cost \n","    \"\"\"\n","\n","    m, n = X.shape\n","    \n","    \n","    fwb=[]\n","    for i in range(m):\n","        yp=sigmoid(np.dot(X[i], w)+b)\n","        fwb.append(yp)\n","        \n","    fwbs=np.array(fwb)\n","    \n","    cost=[]\n","    loss=0\n","    for i in range(m):\n","        loss=-y[i]*np.log(fwbs[i])-(1-y[i])*np.log(1-fwbs[i])\n","        cost.append(loss)\n","        \n","    c=np.array(cost)\n","    total_cost=np.sum(c)/m \n","\n","    return total_cost\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb-FTTu0OGZQ"},"outputs":[],"source":["# m, n = X_train.shape\n","\n","# # Compute and display cost with w and b initialized to zeros\n","# initial_w = np.zeros(len(X_train[0]))\n","# initial_b = 0.\n","# cost = compute_cost(X_train, y_train, initial_w, initial_b)\n","# print('Cost at initial w and b (zeros): {:.3f}'.format(cost))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQQKJGZjTGML"},"outputs":[],"source":["# # Compute and display cost with non-zero w and b\n","# np.random.seed(1)\n","# test_w = 0.01 * (np.random.rand(784) - 0.5)\n","# test_b = -24.\n","# cost = compute_cost(X_train, en[0], test_w, test_b)\n","\n","# print('Cost at test w and b (non-zeros): {:.3f}'.format(cost))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJSWq9XYONQX"},"outputs":[],"source":["# UNQ_C3\n","# GRADED FUNCTION: compute_gradient\n","def compute_gradient(X, y, w, b, *argv): \n","    \"\"\"\n","    Computes the gradient for logistic regression \n"," \n","    Args:\n","      X : (ndarray Shape (m,n)) data, m examples by n features\n","      y : (ndarray Shape (m,))  target value \n","      w : (ndarray Shape (n,))  values of parameters of the model      \n","      b : (scalar)              value of bias parameter of the model\n","      *argv : unused, for compatibility with regularized version below\n","    Returns\n","      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n","      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n","    \"\"\"\n","    m, n = X.shape\n","    dj_dw = np.zeros(w.shape)\n","    dj_db = 0.\n","    djn=np.zeros(1)\n","\n","    ### START CODE HERE ### \n","    fwb=[]\n","    for i in range(m):\n","        yp=sigmoid(np.dot(X[i], w)+b)\n","        fwb.append(yp)\n","        \n","    fwbs=np.array(fwb)\n","    \n","    for j in range(n):    \n","        for i in range(m):\n","            dj_dw[j]+=(fwbs[i]-y[i])*X[i][j]\n","    dj_dw=dj_dw/m\n","    \n","    dj_db=(np.sum(fwbs-y))/m\n","            \n","    \n","    ### END CODE HERE ###\n","\n","        \n","    return dj_db, dj_dw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlYJNRSsOSqA"},"outputs":[],"source":["# # Compute and display gradient with w and b initialized to zeros\n","# initial_w = np.zeros(len(X_train[0]))\n","# initial_b = 0.\n","\n","# dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n","# print(f'dj_db at initial w and b (zeros):{dj_db}' )\n","# print(f'dj_dw at initial w and b (zeros):{dj_dw.tolist()}' )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yYoCHM5VZJp"},"outputs":[],"source":["# # Compute and display cost and gradient with non-zero w and b\n","# np.random.seed(1)\n","# test_w = 0.01 * (np.random.rand(784) - 0.5)\n","# test_b = -24\n","# dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n","\n","# print('dj_db at test w and b:', dj_db)\n","# print('dj_dw at test w and b:', dj_dw.tolist())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvZ3UF26PST-"},"outputs":[],"source":["def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n","    \"\"\"\n","    Performs batch gradient descent to learn theta. Updates theta by taking \n","    num_iters gradient steps with learning rate alpha\n","    \n","    Args:\n","      X :    (ndarray Shape (m, n) data, m examples by n features\n","      y :    (ndarray Shape (m,))  target value \n","      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n","      b_in : (scalar)              Initial value of parameter of the model\n","      cost_function :              function to compute cost\n","      gradient_function :          function to compute gradient\n","      alpha : (float)              Learning rate\n","      num_iters : (int)            number of iterations to run gradient descent\n","      lambda_ : (scalar, float)    regularization constant\n","      \n","    Returns:\n","      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n","          running gradient descent\n","      b : (scalar)                Updated value of parameter of the model after\n","          running gradient descent\n","    \"\"\"\n","    \n","    # number of training examples\n","    m = len(X)\n","    \n","    # An array to store cost J and w's at each iteration primarily for graphing later\n","    J_history = []\n","    w_history = []\n","    \n","    for i in range(num_iters):\n","\n","        # Calculate the gradient and update the parameters\n","        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n","\n","        # Update Parameters using w, b, alpha and gradient\n","        w_in = w_in - alpha * dj_dw               \n","        b_in = b_in - alpha * dj_db              \n","       \n","        # Save cost J at each iteration\n","        if i<100000:      # prevent resource exhaustion \n","            cost =  cost_function(X, y, w_in, b_in, lambda_)\n","            J_history.append(cost)\n","\n","        # Print cost every at intervals 10 times or as many iterations if < 10\n","        if i% math.ceil(num_iters/2) == 0 or i == (num_iters-1):\n","            w_history.append(w_in)\n","            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n","        \n","    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLnukESSPhcm"},"outputs":[],"source":["#Initialising ultravariables\n","np.random.seed(1)\n","initial_w = 0.01 * (np.random.rand(784) - 0.5)\n","initial_b = -8"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGGTfUsQbGRp","executionInfo":{"status":"ok","timestamp":1680251396450,"user_tz":-330,"elapsed":3222390,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}},"outputId":"f804dc50-092b-4a43-b477-25dd368c7284"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration    0: Cost     0.53   \n","Iteration   25: Cost     0.21   \n","Iteration   49: Cost     0.19   \n","Iteration    0: Cost     0.59   \n","Iteration   25: Cost     0.10   \n","Iteration   49: Cost     0.08   \n","Iteration    0: Cost     0.48   \n","Iteration   25: Cost     0.27   \n","Iteration   49: Cost     0.24   \n","Iteration    0: Cost     0.57   \n","Iteration   25: Cost     0.20   \n","Iteration   49: Cost     0.18   \n","Iteration    0: Cost     0.44   \n","Iteration   25: Cost     0.23   \n","Iteration   49: Cost     0.19   \n","Iteration    0: Cost     0.74   \n","Iteration   25: Cost     0.45   \n","Iteration   49: Cost     0.39   \n","Iteration    0: Cost     0.54   \n","Iteration   25: Cost     0.38   \n","Iteration   49: Cost     0.36   \n","Iteration    0: Cost     0.67   \n","Iteration   25: Cost     0.14   \n","Iteration   49: Cost     0.12   \n","Iteration    0: Cost     0.52   \n","Iteration   25: Cost     0.15   \n","Iteration   49: Cost     0.12   \n","Iteration    0: Cost     0.53   \n","Iteration   25: Cost     0.08   \n","Iteration   49: Cost     0.07   \n"]}],"source":["#Making the prediction matrix.\n","pred=np.zeros((10,len(X_train)))\n","for i in range(10):\n","  iterations =50\n","  alpha = 0.2\n","  w,b, J_history,_ = gradient_descent(X_train ,en[i], initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations, 0)\n","  \n","  #Creating the probability matrix for each class\n","  fwb=[]\n","  m=len(X_train)\n","  for j in range(m):\n","    yp=sigmoid(np.dot(X_train[j], w)+b)\n","    fwb.append(yp)\n","        \n","  fwbs=np.array(fwb)\n","  pred[i]=fwbs\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"MW4xP4sxnI5K","executionInfo":{"status":"ok","timestamp":1680251396453,"user_tz":-330,"elapsed":9,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}}},"outputs":[],"source":["#final prediction\n","predict=pred.transpose()\n","y_final=[]\n","\n","for i in predict:\n","  l=0\n","  for m in range(10):    \n","    if i[l]==max(i):\n","      y_final.append(l)\n","      break\n","    else:\n","      l+=1\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"oocrOpqKsDki","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680251396454,"user_tz":-330,"elapsed":10,"user":{"displayName":"Shivansh Anand","userId":"12682862733612927854"}},"outputId":"62a38a5b-fd65-48b5-b855-8008c3108cce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Accuracy: 74.463333\n"]}],"source":["print('Train Accuracy: %f'%(np.mean(y_final == y_train) * 100))"]},{"cell_type":"markdown","metadata":{"id":"u0KqyGsZsFuJ"},"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkSKOyKzmLy/V9qU/0qEZR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}